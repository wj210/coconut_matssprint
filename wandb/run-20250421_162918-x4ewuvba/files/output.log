Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1655.99 examples/s]
Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 26245.81 examples/s]
Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1607.43 examples/s]
Training Epoch: 1/5, batch 156/157 completed (loss: 0.0534: 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 157/157 [08:59<00:00,  3.44s/it]
logging training data
/export/home2/weijie210/miniconda3/envs/cot/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.43it/s]
saving model.
Traceback (most recent call last):
  File "/export/home2/weijie210/coconut/run.py", line 585, in <module>
    main()
  File "/export/home2/weijie210/coconut/run.py", line 435, in main
    del states
UnboundLocalError: local variable 'states' referenced before assignment
[rank0]: Traceback (most recent call last):
[rank0]:   File "/export/home2/weijie210/coconut/run.py", line 585, in <module>
[rank0]:     main()
[rank0]:   File "/export/home2/weijie210/coconut/run.py", line 435, in main
[rank0]:     del states
[rank0]: UnboundLocalError: local variable 'states' referenced before assignment
