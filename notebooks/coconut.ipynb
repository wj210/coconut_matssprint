{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec49ebf",
   "metadata": {},
   "source": [
    "# Eval coconut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd45e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from coconut import Coconut\n",
    "from dataset import (\n",
    "    get_dataset,\n",
    "    get_question_latent_dataset,\n",
    "    get_cot_latent_dataset,\n",
    "    MyCollator,\n",
    "    collate_and_add_latent\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from utils import Config, set_seed\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5137ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2241507/873421467.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_weights = torch.load(load_model_path, map_location=device)\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coconut = True\n",
    "cot=False\n",
    "no_thoughts=False\n",
    "no_cot = False\n",
    "device = 'cuda'\n",
    "model_path = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map = device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens(\"<|start-latent|>\")\n",
    "tokenizer.add_tokens(\"<|end-latent|>\")\n",
    "tokenizer.add_tokens(\"<|latent|>\")\n",
    "latent_id = tokenizer.convert_tokens_to_ids(\"<|latent|>\")\n",
    "start_id = tokenizer.convert_tokens_to_ids(\"<|start-latent|>\")\n",
    "end_id = tokenizer.convert_tokens_to_ids(\"<|end-latent|>\")\n",
    "\n",
    "load_model_path = \"checkpoint/cladder-coconut_gen_llama_1B/checkpoint_8\"\n",
    "# load_model_path = \"checkpoint/cladder-cot_gen_llama_1B/checkpoint_3\"\n",
    "saved_weights = torch.load(load_model_path, map_location=device)\n",
    "\n",
    "if coconut:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    target_id = tokenizer.convert_tokens_to_ids(\"<<\")\n",
    "    # initialize the new token embeddings with a known token\n",
    "    # it helps stablize the training\n",
    "    for token_id in [latent_id, start_id, end_id]:\n",
    "        target_embedding = embeddings.weight.data[token_id]\n",
    "        embeddings.weight.data[token_id] = target_embedding\n",
    "        # The input embeddings and lm heads are tied in GPT2. So the code below is not necessary\n",
    "        lm_head = model.lm_head\n",
    "        lm_head.weight.data[token_id] = lm_head.weight.data[target_id]\n",
    "\n",
    "    # load model\n",
    "    model = Coconut(model, latent_id, start_id, end_id, tokenizer.eos_token_id)\n",
    "model.load_state_dict(saved_weights, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a135dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if coconut:\n",
    "    num_latent_tokens = 6\n",
    "    c_thought = 1\n",
    "else:\n",
    "    num_latent_tokens = 0\n",
    "    c_thought = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1cec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a325da6ac4243928d9337d1e746d0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c525a1fe8a834c8989bba7da1a6621a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_path = \"data/cladder_test_gen.json\"\n",
    "n_samples = 300\n",
    "ds = get_dataset(test_path, tokenizer)\n",
    "ds = ds.select(range(n_samples))\n",
    "original_ds = json.load(open(test_path))\n",
    "original_ds = original_ds[:n_samples]\n",
    "configs = Config({'pad_latent_to_max':True,'max_latent_stage':num_latent_tokens,'c_thought':c_thought})\n",
    "test_ds = get_question_latent_dataset(\n",
    "        num_latent_tokens,\n",
    "        ds,\n",
    "        configs,\n",
    "        start_id,\n",
    "        latent_id,\n",
    "        end_id,\n",
    "        no_special_marker= not coconut,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bc9b0",
   "metadata": {},
   "source": [
    "Get base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d9405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_perf(ds,compare_step=False):\n",
    "    acc = []\n",
    "    acc_step = []\n",
    "    for i,batch in tqdm(enumerate(ds),total=len(ds)):\n",
    "        idx = batch['idx']\n",
    "        step = original_ds[idx]['steps']\n",
    "        last_step = step[-1].split(',')[-1].strip()\n",
    "        ans = original_ds[idx]['answer'].strip().lower()\n",
    "        ans_id = tokenizer.encode(' '+ ans, add_special_tokens=False)[0] # add space to the start of the answer\n",
    "        batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items() if v != None and k not in [\"idx\", \"position_ids\"]}\n",
    "        if coconut:\n",
    "            outputs,output_logits = model.generate(**batch,max_new_tokens=10 if num_latent_tokens == 6 else 50,return_logits = True) # more token if language steps\n",
    "            ans_prob = output_logits.to(device).softmax(-1)[-1,ans_id].item() # prob of the answer\n",
    "        else:\n",
    "            outputs = model.generate(**batch,max_new_tokens=200,pad_token_id=tokenizer.eos_token_id)\n",
    "        if compare_step: # only possible if num_latent_tokens < 6\n",
    "            pred_step = pred.split('#')[0].split(',')[-1].strip()\n",
    "            acc_step.append(pred_step == last_step) # compare step as well. (answer is only binary.)\n",
    "\n",
    "        pred = tokenizer.decode(outputs[0, batch['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        pred_ans = pred.split('#')[-1].strip().lower()\n",
    "        acc.append(pred_ans == ans)\n",
    "    return np.mean(acc), None if not compare_step else np.mean(acc_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8ac126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:51<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc,_ = get_perf(test_ds,compare_step=False)\n",
    "print (f'Base Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d809aa4",
   "metadata": {},
   "source": [
    "What happens if we instead ablate the number of latent tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e40993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9090dd4eb594ec59fbd5d387bfbaae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:27<00:00, 11.06it/s]\n",
      " 17%|████████████████▏                                                                                | 1/6 [00:28<02:22, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 0 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97e6d497cd45e4b8d68e8373c8aa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "num_latent_range = range(num_latent_tokens)\n",
    "for num_latent in tqdm(num_latent_range,total = len(num_latent_range)): # change number of latent tokens given.\n",
    "    configs = Config({'pad_latent_to_max':True,'max_latent_stage':num_latent,'c_thought':c_thought})\n",
    "    test_ds = get_question_latent_dataset(\n",
    "            num_latent,\n",
    "            ds,\n",
    "            configs,\n",
    "            start_id,\n",
    "            latent_id,\n",
    "            end_id,\n",
    "            no_special_marker= not coconut, # if 0, no special marker as well\n",
    "        )\n",
    "    acc,_ = get_perf(test_ds,compare_step=False)\n",
    "    print (f'Accuracy with {num_latent} latent tokens: {acc:.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latents first\n",
    "all_latents = []\n",
    "for i,batch in tqdm(enumerate(test_ds), total=len(test_ds)):\n",
    "    batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items()}\n",
    "    idx = batch['idx'].item()\n",
    "    steps = original_ds[idx]['steps']\n",
    "    batch['labels'] = batch['input_ids'].clone()\n",
    "    output = model(**batch,store_latent=True)\n",
    "    all_latents.append(output.latents)\n",
    "\n",
    "    # for i in range(output.latents.shape[1]): # cosine similarity\n",
    "    #     print (torch.nn.functional.cosine_similarity(output.latents[0,i], output.latents[0,-1], dim=-1))\n",
    "    # break\n",
    "    # decoded_latent = []\n",
    "    # for latent in output.latents[0]:\n",
    "    #     logits = latent @ model.base_causallm.lm_head.weight.T\n",
    "    #     decoded_latent.append((tokenizer.decode(logits.argmax(dim = 0))))\n",
    "\n",
    "\n",
    "all_latents = torch.cat(all_latents, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1e740",
   "metadata": {},
   "source": [
    "See what would happen if we intervene and insert a random latent at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad4d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [04:54<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "step_acc = defaultdict(list)\n",
    "step_diff = []\n",
    "for i,batch in tqdm(enumerate(test_ds),total=len(test_ds)):\n",
    "    idx = batch['idx']\n",
    "    rung = original_ds[idx]['rung']\n",
    "    step = original_ds[idx]['steps'][-1].split(',')[-1].strip()\n",
    "    ans = original_ds[idx]['answer'].strip().lower()\n",
    "    ans_id = tokenizer.encode(' '+ ans, add_special_tokens=False)[0] # add space to the start of the answer\n",
    "    batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items() if v != None and k not in [\"idx\", \"position_ids\"]}\n",
    "\n",
    "    random_idx = i\n",
    "    while random_idx == i: # make sure we don't sample the same latent\n",
    "        random_idx = np.random.randint(0,all_latents.shape[0],(1,)).item()\n",
    "    for step_idx in range(num_latent_tokens):\n",
    "        sample_latent = all_latents[random_idx]\n",
    "        outputs = model.generate(**batch,max_new_tokens=10,insert_latent = sample_latent,insert_step=step_idx)\n",
    "        pred = tokenizer.decode(outputs[0, batch['input_ids'].shape[1]:])\n",
    "        pred_ans = pred.split('#')[-1].strip().lower()\n",
    "        step_acc[step_idx].append(pred_ans == ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05eec5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ans acc: 0.74\n",
      "Step 1: ans acc: 0.74\n",
      "Step 2: ans acc: 0.74\n",
      "Step 3: ans acc: 0.74\n",
      "Step 4: ans acc: 0.74\n",
      "Step 5: ans acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "for step,scores in step_acc.items():\n",
    "    acc = sum(scores) / len(scores)\n",
    "    print(f\"Step {step}: ans acc: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badd601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
