{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec49ebf",
   "metadata": {},
   "source": [
    "# Eval coconut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd45e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from coconut import Coconut\n",
    "from dataset import (\n",
    "    get_dataset,\n",
    "    get_question_latent_dataset,\n",
    "    get_cot_latent_dataset,\n",
    "    MyCollator,\n",
    "    collate_and_add_latent\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from utils import Config, set_seed\n",
    "import numpy as np\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5137ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d43d476308c416a9a289a7caee83fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3e45f194424fb683d68200cfd66661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4a1e5d404848d9b62b05cd5a9e70cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04b5d3274da46bf886813d4079270cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84544f11f9d943d6a5fea2e3c8173a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da4d9a84f364f00b252b245c40e234d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coconut = True\n",
    "cot=False\n",
    "no_thoughts=False\n",
    "no_cot = False\n",
    "device = 'cuda'\n",
    "model_path = \"meta-llama/Llama-3.2-1B\"\n",
    "gen = False # if want to test the generalizing model.\n",
    "load_from_hf = True\n",
    "\n",
    "if load_from_hf:\n",
    "    model_path = \"weijie210/Llama-3.2-1B_cladder_6\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map = device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens(\"<|start-latent|>\")\n",
    "tokenizer.add_tokens(\"<|end-latent|>\")\n",
    "tokenizer.add_tokens(\"<|latent|>\")\n",
    "latent_id = tokenizer.convert_tokens_to_ids(\"<|latent|>\")\n",
    "start_id = tokenizer.convert_tokens_to_ids(\"<|start-latent|>\")\n",
    "end_id = tokenizer.convert_tokens_to_ids(\"<|end-latent|>\")\n",
    "\n",
    "if gen:\n",
    "    if coconut:\n",
    "        load_model_path = \"checkpoint/cladder-coconut_gen_llama_1B/checkpoint_8\" # use checkpoint_5 for latent 5, 1 natural langauge step\n",
    "    else:\n",
    "        load_model_path = \"checkpoint/cladder-cot_gen_llama_1B/checkpoint_3\"\n",
    "else:\n",
    "    if coconut:\n",
    "        load_model_path = \"checkpoint/cladder-coconut_llama_1B/checkpoint_8\"\n",
    "    else:\n",
    "        load_model_path = \"checkpoint/cladder-cot_llama_1B/checkpoint_12\"\n",
    "\n",
    "\n",
    "if coconut:\n",
    "    if not load_from_hf:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        embeddings = model.get_input_embeddings()\n",
    "        target_id = tokenizer.convert_tokens_to_ids(\"<<\")\n",
    "        # initialize the new token embeddings with a known token\n",
    "        # it helps stablize the training\n",
    "        for token_id in [latent_id, start_id, end_id]:\n",
    "            target_embedding = embeddings.weight.data[token_id]\n",
    "            embeddings.weight.data[token_id] = target_embedding\n",
    "            # The input embeddings and lm heads are tied in GPT2. So the code below is not necessary\n",
    "            lm_head = model.lm_head\n",
    "            lm_head.weight.data[token_id] = lm_head.weight.data[target_id]\n",
    "\n",
    "    # load model\n",
    "    model = Coconut(model, latent_id, start_id, end_id, tokenizer.eos_token_id)\n",
    "if not load_from_hf:\n",
    "    saved_weights = torch.load(load_model_path, map_location=device)\n",
    "    model.load_state_dict(saved_weights, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a135dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if coconut:\n",
    "    num_latent_tokens = 6\n",
    "    c_thought = 1\n",
    "else:\n",
    "    num_latent_tokens = 0\n",
    "    c_thought = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1cec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85adbf82b674136a0831929aa89ae66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1194b26094c844638be742faca1a9c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if gen:\n",
    "    test_path = \"data/cladder_test_gen.json\" \n",
    "else:\n",
    "    test_path = \"data/cladder_test.json\"\n",
    "n_samples = 300\n",
    "ds = get_dataset(test_path, tokenizer)\n",
    "ds = ds.select(range(n_samples))\n",
    "original_ds = json.load(open(test_path))\n",
    "original_ds = original_ds[:n_samples]\n",
    "configs = Config({'pad_latent_to_max':True,'max_latent_stage':num_latent_tokens,'c_thought':c_thought})\n",
    "test_ds = get_question_latent_dataset(\n",
    "        num_latent_tokens,\n",
    "        ds,\n",
    "        configs,\n",
    "        start_id,\n",
    "        latent_id,\n",
    "        end_id,\n",
    "        no_special_marker= not coconut,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385bc9b0",
   "metadata": {},
   "source": [
    "Get base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d9405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_perf(ds,compare_step=False):\n",
    "    acc = []\n",
    "    acc_step = []\n",
    "    for i,batch in tqdm(enumerate(ds),total=len(ds)):\n",
    "        idx = batch['idx']\n",
    "        step = original_ds[idx]['steps']\n",
    "        last_step = step[-1].split(',')[-1].strip()\n",
    "        ans = original_ds[idx]['answer'].strip().lower()\n",
    "        ans_id = tokenizer.encode(' '+ ans, add_special_tokens=False)[0] # add space to the start of the answer\n",
    "        batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items() if v != None and k not in [\"idx\", \"position_ids\"]}\n",
    "        if coconut:\n",
    "            outputs,output_logits = model.generate(**batch,max_new_tokens=10 if num_latent_tokens == 6 else 50,return_logits = True) # more token if language steps\n",
    "            ans_prob = output_logits.to(device).softmax(-1)[-1,ans_id].item() # prob of the answer\n",
    "        else:\n",
    "            outputs = model.generate(**batch,max_new_tokens=200,pad_token_id=tokenizer.eos_token_id)\n",
    "        if compare_step: # only possible if num_latent_tokens < 6\n",
    "            pred_step = pred.split('#')[0].split(',')[-1].strip()\n",
    "            acc_step.append(pred_step == last_step) # compare step as well. (answer is only binary.)\n",
    "\n",
    "        pred = tokenizer.decode(outputs[0, batch['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        pred_ans = pred.split('#')[-1].strip().lower()\n",
    "        acc.append(pred_ans == ans)\n",
    "    return np.mean(acc), None if not compare_step else np.mean(acc_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8ac126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/300 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:49<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc,_ = get_perf(test_ds,compare_step=False)\n",
    "print (f'Base Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d809aa4",
   "metadata": {},
   "source": [
    "What happens if we instead ablate the number of latent tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e40993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9090dd4eb594ec59fbd5d387bfbaae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:27<00:00, 11.06it/s]\n",
      " 17%|████████████████▏                                                                                | 1/6 [00:28<02:22, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 0 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97e6d497cd45e4b8d68e8373c8aa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:31<00:00,  9.52it/s]\n",
      " 33%|████████████████████████████████▎                                                                | 2/6 [01:01<02:03, 30.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 1 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad661edac4344a2897e63438fd58a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:35<00:00,  8.46it/s]\n",
      " 50%|████████████████████████████████████████████████▌                                                | 3/6 [01:37<01:40, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 2 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c191a5d684f48a39a700e2bc9f1899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:39<00:00,  7.56it/s]\n",
      " 67%|████████████████████████████████████████████████████████████████▋                                | 4/6 [02:18<01:13, 36.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 3 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4145c561c4d442a8a8ea5785f22bc6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:43<00:00,  6.83it/s]\n",
      " 83%|████████████████████████████████████████████████████████████████████████████████▊                | 5/6 [03:04<00:39, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 4 latent tokens: 0.42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cd0fb0e16a4fdaa0670c90320547d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:48<00:00,  6.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:53<00:00, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 5 latent tokens: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_latent_range = range(num_latent_tokens)\n",
    "for num_latent in tqdm(num_latent_range,total = len(num_latent_range)): # change number of latent tokens given.\n",
    "    configs = Config({'pad_latent_to_max':True,'max_latent_stage':num_latent,'c_thought':c_thought})\n",
    "    test_ds = get_question_latent_dataset(\n",
    "            num_latent,\n",
    "            ds,\n",
    "            configs,\n",
    "            start_id,\n",
    "            latent_id,\n",
    "            end_id,\n",
    "            no_special_marker= not coconut, # if 0, no special marker as well\n",
    "        )\n",
    "    acc,_ = get_perf(test_ds,compare_step=False)\n",
    "    print (f'Accuracy with {num_latent} latent tokens: {acc:.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latents first\n",
    "all_latents = []\n",
    "for i,batch in tqdm(enumerate(test_ds), total=len(test_ds)):\n",
    "    batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items()}\n",
    "    idx = batch['idx'].item()\n",
    "    steps = original_ds[idx]['steps']\n",
    "    batch['labels'] = batch['input_ids'].clone()\n",
    "    output = model(**batch,store_latent=True)\n",
    "    all_latents.append(output.latents)\n",
    "\n",
    "    # for i in range(output.latents.shape[1]): # cosine similarity\n",
    "    #     print (torch.nn.functional.cosine_similarity(output.latents[0,i], output.latents[0,-1], dim=-1))\n",
    "    # break\n",
    "    # decoded_latent = []\n",
    "    # for latent in output.latents[0]:\n",
    "    #     logits = latent @ model.base_causallm.lm_head.weight.T\n",
    "    #     decoded_latent.append((tokenizer.decode(logits.argmax(dim = 0))))\n",
    "\n",
    "\n",
    "all_latents = torch.cat(all_latents, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1e740",
   "metadata": {},
   "source": [
    "See what would happen if we intervene and insert a random latent at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [04:54<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "step_acc = defaultdict(list)\n",
    "step_diff = []\n",
    "for i,batch in tqdm(enumerate(test_ds),total=len(test_ds)):\n",
    "    idx = batch['idx']\n",
    "    rung = original_ds[idx]['rung']\n",
    "    step = original_ds[idx]['steps'][-1].split(',')[-1].strip()\n",
    "    ans = original_ds[idx]['answer'].strip().lower()\n",
    "    ans_id = tokenizer.encode(' '+ ans, add_special_tokens=False)[0] # add space to the start of the answer\n",
    "    batch = {k: torch.tensor(v).to(device).unsqueeze(0) for k, v in batch.items() if v != None and k not in [\"idx\", \"position_ids\"]}\n",
    "\n",
    "    random_idx = i\n",
    "    while random_idx == i: # make sure we don't sample the same latent\n",
    "        random_idx = np.random.randint(0,all_latents.shape[0],(1,)).item()\n",
    "    for step_idx in range(num_latent_tokens):\n",
    "        sample_latent = all_latents[random_idx]\n",
    "        outputs = model.generate(**batch,max_new_tokens=10,insert_latent = sample_latent,insert_step=step_idx)\n",
    "        pred = tokenizer.decode(outputs[0, batch['input_ids'].shape[1]:])\n",
    "        pred_ans = pred.split('#')[-1].strip().lower()\n",
    "        step_acc[step_idx].append(pred_ans == ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eec5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: ans acc: 0.74\n",
      "Step 1: ans acc: 0.74\n",
      "Step 2: ans acc: 0.74\n",
      "Step 3: ans acc: 0.74\n",
      "Step 4: ans acc: 0.74\n",
      "Step 5: ans acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "for step,scores in step_acc.items():\n",
    "    acc = sum(scores) / len(scores)\n",
    "    print(f\"Step {step}: ans acc: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74b97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc87407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da058c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e02864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a91698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc96e549",
   "metadata": {},
   "source": [
    "# save HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_folder\n",
    "\n",
    "model_name = \"Llama-3.2-1B_cladder_6\"\n",
    "local_dir = \"checkpoint/Llama-3.2-1B_cladder_6\"\n",
    "model.base_causallm.save_pretrained(local_dir)\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "\n",
    "create_repo(model_name, repo_type=\"model\", exist_ok=True)\n",
    "upload_folder(\n",
    "    repo_id=f\"weijie210/{model_name}\",\n",
    "    folder_path=local_dir,\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
