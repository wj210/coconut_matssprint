# need 4 gpus

project: coconut
save_path: checkpoint
name: cladder-cot_gen_llama_1B # gen means train on rung 1/2 and test on 3

only_eval: False

coconut: False
cot: True
no_thoughts: False
no_cot: False

c_thought: 0
epochs_per_stage: 1
max_latent_stage: 0
pad_latent_to_max: True
train_size: 10000
val_size: 100

save_only_improve: True
uniform_prob: 0.0
model_id: "meta-llama/Llama-3.2-1B"
load_model_path: None
seed: 0
resume: 0
bf16: False
train_path: data/cladder_train_gen.json
val_path: data/cladder_valid_gen.json
reset_optimizer: False
batch_size_training: 4
debug: False
gradient_accumulation_steps: 1
num_epochs: 15
lr: !!float "2e-5" # higher for LORA
weight_decay: 0.01
